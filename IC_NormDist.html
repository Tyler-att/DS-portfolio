<!DOCTYPE HTML>
<!--
    Read Only by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
    <title>Image Classification: Probability Distributions</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body class="is-preload">

    <!-- Header -->
    <section id="header">
        <header>
            <span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
            <h1 id="logo"><a href="index.html">Tyler Watt</a></h1>
            <p>
                I'm not sure what the most useful field of Mathematics is,<br />
                but it's probably Statistics
            </p>
        </header>
        <nav id="nav">
            <ul>
                <li><a href="#PreliminaryIdea" class="active">Preliminary Idea</a></li>
                <li><a href="#CreatingData">Creating the Data</a></li>
                <li><a href="#TheModel">The Model</a></li>
                <li><a href="#PostMortem">Post Mortem</a></li>
            </ul>
        </nav>
        <footer>
            <ul class="icons">
                <li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
                <li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
                <li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
                <li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
                <li><a href="#" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
            </ul>
        </footer>
    </section>

    <!-- Wrapper -->
    <div id="wrapper">

        <!-- Main -->
        <div id="main">

            <!-- One -->
            <section id="PreliminaryIdea">
                <div class="image main" data-position="center">
                    <img src="images/Seven Distribution Skews.png" height = "300" alt="Seven Distributions Fomatted" />
                </div>
                <div class="container">
                    <header class="major">
                        <h2>Image Classification of Probability Distributions</h2>
                        <p>
                            Preliminary Idea
                        </p>
                    </header>
                    <p>
                        <font size="+2">TL;DR</font>
                        <br />
                        I make a Convolutional Neural Network that can classify images of histograms into seven levels of skew: severe left, moderate left, mild left, normal,
                        mild right, moderate right, and severe right with a 94.81% accuracy.
                        <br />
                        <br />
                        <font size="+2">Initial Thoughts</font>
                        <br />
                        One word of warning: this section will be very long and will mostly just outline the problem and go through my thought process for how to solve it. If you 
                        are interested in seeing how I approach solving problems like these, it might be an interesting read. However, if you’re just here to see the algorithms
                        and statistics, feel free to skip down to <a href="#CreatingData">here</a>.
                        <br />
                        <br />
                        I’m writing this portion of the project prior to starting it. I figured it would be a good idea to capture my thoughts on how I plan on making this project
                        work to be able to contrast how it actually ends up working. Hopefully this should be good fun to read for the AI veterans who will likely already know the
                        hubris of this plan, and it’s a good way to document the naïve optimism of a simple data science enthusiast with delusions of grandeur. As I am sure those
                        will be promptly burned to ashes, I recommend bringing marshmallows.
                        <br />
                        <br />
                        So enough with my weird self-detrimental style of humor, what’s the idea? In two words: distribution classification. I plan on using TensorFlow to create an
                        image classifier that will identify the distribution of a dataset based on its histogram. At the end of it, I am hoping I can give it a histogram of any
                        distribution at all, and it will tell me whether it is normally distributed or skewed to some degree in some direction.
                        <br />
                        <br />
                        So I guess the first question is: why? What value does a distribution identification AI give us, especially when statistical tests already exist for such
                        things (e.g. Anderson-Darling test for normality)?
                        <br />
                        Well for starters, the Anderson-Darling test for normality is very good at its job. Some argue that it may be too good for some purposes. Most if not all
                        statistical tests have an inherent amount of power based on sample size and various other factors. This means that the larger the sample size, the more
                        likely a test will be able to identify a deviation from the null hypothesis, no matter how small. In most cases, this is a good thing, as it reduces the
                        probability of having a type II error. However, for distribution identification, this may not be for the best. Some of the most common and useful
                        statistical tests are parametric and rely on the dataset to be normally distributed. However, some distributions are functionally normal, meaning that they
                        may have slight deviations from the normal distribution, but they will work just fine for most statistical tests. The issue here is if I have a few
                        thousand data points (or millions, as many data science problems have), the power of the Anderson-Darling test will be so high that only the most pristine,
                        beautifully normal datasets will make the cut. For example, the following image shows a distribution that fails the Anderson-Darling test at the 95%
                        confidence level, and it only has 1000 measurements.
                        <br />
                        <img src="images/Barely_Non_Normal.png" height="250" width="250" alt="Barely Non-Normal">
                        <br />
                        So all that to be said that my hope and dream for this AI is that it is worse at identifying distributions than the normally used statistical techniques.
                        All jokes aside, my inspiration for this comes from how the assumptions for linear regression are normally assessed. It’s rare to actually apply an actual
                        normality test to the residuals of a model. Often if we do anything at all, we look at the histogram or the normality plot and say, “yeah, that’s about 
                        right” and go from there. In the figure above, I'd be ecstatic to have such a nice looking dataset.
                        <br />
                        The second reason is I have plans for an AI equivalent to the Johnson transformation, and I think it would be interesting to have it be able to assess its
                        own work by looking at the histograms it creates.
                        <br />
                        <br />
                        <font size="+2">Potential Pitfalls</font>
                        <br />
                        So before even starting this project, I see a few obstacles I’m going to have to get around. The first is converting the image into something useable. All
                        the “hello world” level image identification projects use tiny thumbnail sized pictures with so few pixels even that I have difficulty classifying them. Is
                        that a shoe, some pants, bigfoot? Who knows? (Actually, I guess the AI knows . . .).
                        <br />
                        <img src="images/Keras_Shoe.png" height="200" width="200" alt="Keras Shoe">
                        <br />
                        The main benefit of this is the neural network only needs a few dozen or so input neurons instead of 250,000. This significantly improves the efficiency of
                        the model and reduces the chances of some overfitting issues. Because the universe is rarely as generous as Keras when it comes to providing easily usable
                        datasets, I will need to figure out a way to handle larger images. I know there are plenty of ways to do this, so I will just need to figure out which one
                        does so without throwing away too much information. My initial thoughts include partitioning each image into smaller, easier to use sections, though I’m
                        also wondering if I can find a way to convert each image into relative bar height and location, as I imagine most of the actual pixel data in the
                        histograms will be redundant for an AI.
                        <br />
                        <br />
                        Another issue I foresee is developing the training set. More specifically, where to store it. Image classification can require massive training sets to get
                        a good fit, and I don’t have space on my computer for 20,000 or more jpgs of various distributions. I may have to figure out compressing the images, using
                        alternative formats, or even creating each image and then immediately destroying it after training, though I imagine that would be unusably slow.
                        This is less of an obstacle and more of a decision, but I need to figure out how I will be actually creating these thousands of histograms. I suppose the
                        easiest would be to use any one of the various methods that Python, R, and Excel have to create various distributions. One alternative would be to simply
                        make three very large datasets, one right-skewed, one left-skewed, and one normal. Then, I could simply leverage the central limit theorem and create
                        variously skewed datasets.
                        <br />
                        <br />
                        That second option actually might have some benefits to it. You see, I’m planning on breaking this out into stages. The initial AI’s output will be
                        entirely binary, either normal or not normal. The second stage will be a little more specific, and will output skewed left, skewed right, or normal. If I’m
                        daring, I may even include some adverbs in there (e.g. mildly, moderately, severely). Creating datasets for this purpose would actually be quite easy.
                        For example, if I wanted to make a bunch of moderately right-skewed data, I could simply take a bunch of n = 5 samples from the severely right-skewed data.
                        Then, if I wanted mildly right-skewed data, I could take a bunch of n = 10 or so right-skewed data samples. If I wanted to go the extra mile, I could even
                        perform an Anderson-Darling test on each of them to verify they are not normally distributed in case it isn’t obvious from the histograms.

                    </p>
                </div>
            </section>

            <!-- Two -->
            <section id="CreatingData">
                <div class="container">
                    <h3>Creating the Data</h3>

                    <p>
                        OK, enough stalling. Data science is quite challening without the former, so we need to create a dataset. Because image classification takes a bit of work,
                        we are going to need somewhere in the thousands of images at least. I don't know of any online sources for such a dataset, so I will try to make one
                        myself.
                        <br />
                        First let's get all the imports out of the way. Here is everything I will end up using for this project.
<pre><code>import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
from random import sample
from statistics import mean
from cv2 import imread, imwrite, imshow, waitKey</code></pre>

                        I think the idea of using three distributions and leveraging the Central Limit Theorem to get various levels of skew should work out best. Numpy's random
                        function is a good fit for this, since it has a wide variety of possible distributions to pull from. I think the beta and F distributions should be good
                        for the left-skewed and right-skewed images, respectively.
                        <br /><br />
<pre><code>normal = list(np.random.normal(0, 1, 10000))
severe_left_skew = list(np.random.beta(7, 1, 10000))
severe_right_skew = list(np.random.f(2, 50, 10000))</code></pre>

                        From here, it's simply a matter of taking random samples from these distributions to create the correct "level" of skew.
                        <br /><br />
<pre><code>skew_list = [('severe_left', severe_left_skew, 2),
('moderate_left', severe_left_skew, 5),
('mild_left', severe_left_skew, 8),
('normal', normal, 1),
('mild_right', severe_right_skew, 60),
('moderate_right', severe_right_skew, 30),
('severe_right', severe_right_skew, 8)]</code></pre>
                        So to explain, each of these seven tuples represents a given level of skew. For example, the second tuple is the "moderate_left" skew, which is derived by
                        taking 5 samples from the severe_left_skew data in the previous block of code. Each histogram will include 300 data points and 20 bins. Here are all seven
                        skews together.
                        <img src="images/Seven Distribution Skews.png" alt="Seven Distributions Fomatted" />
                        It's at this point I want to point out that the "moderate right" skew is actually an average of 30 data points from a non-normal distribution, which kind
                        of goes against the convention that n = 30 is enough to make a distribution approximately normal. To be fair, it is a little less clear than the moderate
                        left distribution, but even the mild right, which has an n = 60 sample size, has a slight tail.
                        <br />
                        <br />
                        So I can create the histograms, now I have to convert them into images, read them into numpy arrays, store all of them in a file so I don't have to
                        keep recreating the set, and then make sure the data can be loaded back in a tensorflow-friendly manner.
                        <br />
                        One other thing that we need to figure out is storage. I don't have room on my computer for 50 GB of jpgs, and I don't want to accidentally use up all of
                        my storage. Seeing how I will likely need hundreds to thousands of images, I need to make sure I'm careful. Let's look at how much space one histogram
                        takes up.
                        <br /><img src="images/hist100.png" alt="100 DPI histogram" /><br />
                        So this is a 100 dpi image and takes up about 11.3 kB. If I used this size, I would expect to be running through 11 or so MB of data per 1000 images, which
                        I imagine would be fairly slow. Let's see what we can do to drop this down.
                        <br /><img src="images/hist100_black.png" alt="100 DPI histogram BW" /><br />
                        Converting to black and white saves me a kB and also will probably be more useful for the neural network. Win win.
                        <br /><img src="images/hist50_black.png" alt="50 DPI histogram BW" /><br />
                        Dropping down the image size also helps significantly. This image is only 3.77 kB. I doubt the neural network needs to worry about those axes either, so
                        let's see what happens when we drop them.
                        <br /><img src="images/hist50_clean.png" alt="50 DPI histogram BW Clean" /><br />
                        This "clean" figure is only 1.04 kB, meaning the axes account for more memory than the histogram itself! I guess the only question now is how small do I
                        want to make these figures? I could drop them down to 5 DPI if I wanted, but at what point do we start losing something important? One thing I am thinking
                        about is how we probably won't get a very good model by looking at each individual pixel. A more common way to do image classification is to run grids
                        across the image to capture larger features. Since I am going to have seven categories, let's choose a size that gives an x-axis that is divisible by seven.
                        <br /><img src="images/hist_7cols.png" alt="Seven Distributions Fomatted" /><br />
                        So this bad boy is 23 DPI, 577 bytes, and 147 pixels wide. This means we could break this image into seven columns 21 pixels wide. Consider a scenario
                        where you were shown a single column and were told where on the picture that column came from. I think from that information alone, you could at least
                        discern whether the graph was skewed left, right, or normal.
                        <br /><br />
<pre><code>dist_train_data = []
dist_test_data = []

for i in range(100):
    fig = plt.hist([mean(sample(severe_right_skew, 8)) for s in range(300)], bins=20, color='black')
    plt.axis('off')
    plt.savefig('hist.png', dpi=23)
    img = imread('hist.png', 0)
    print(i)
    if i <= 10:
        dist_test_data.append(['severe_right', img])
    else:
        dist_train_data.append(['severe_right', img])

np.savez('dist_data/dist_train_data_6.npz', *dist_train_data)
np.savez('dist_data/dist_test_data_6.npz', *dist_test_data)</code></pre>

                        Now you may notice here that the above code only creates the severe right skew. My original code did this for all seven skews and actually did 500 figures
                        each instead of just 100, but my poor computer couldn't really handle it. Even after updating the algorithm to clear the lists every 100 figures didn't
                        really help. After some thought, I considered that I may not need 500 figures each. Most other projects have gradient pixel values, but mine will be very
                        binary. Black or white. So I ended up splitting into seven very similar for loops that generated 100 figures each.
                        <br />
                        <br />
                        Later I would combine each of these .npz files into larger training and test datasets.
                        <br /><br />
<pre><code>train_labels = []
train_imgs = []
test_labels = []
test_imgs = []

for i in range(7):
    print(i)
    dist = np.load('dist_data/dist_train_data_' + str(i) + '.npz', allow_pickle = True)
for key in dist:
    train_labels.append(dist[key][0])
    train_imgs.append(dist[key][1])

for i in range(7):
    print(i)
    dist = np.load('dist_data/dist_test_data_' + str(i) + '.npz', allow_pickle = True)
for key in dist:
    test_labels.append(dist[key][0])
    test_imgs.append(dist[key][1])

np.savez('dist_train_imgs.npz', *train_imgs)
np.savez('dist_train_lbls.npz', *train_labels)
np.savez('dist_test_imgs.npz', *test_imgs)
np.savez('dist_test_lbls.npz', *test_labels)</code></pre>

                        I guess one notable thing here is that I had stored the datasets as lists, and numpy.load() is not a fan of working with objects. It's actually a useful
                        security thing. Apparently if the system is set to load objects, it can lead to running malware. Fortunately for me, I created all of this data myself, so
                        I don't really have to worry about the whole malware thing.
                        <br /><br />
                        OK, so now I have a dozen or so pickles and four full datasets! Well sort of . . . there's still some
                        final touches we need to get this data in a usable state.
                        <br /><br />
<pre><code># Converting categories to numeric values
label_dict = {'severe_left': 0, 'moderate_left': 1, 'mild_left': 2, 'normal': 3,
'mild_right': 4, 'moderate_right': 5, 'severe_right': 6}

# Creating each dataset, making sure to change labels to numbers.
train_imgs = np.load('dist_train_imgs.npz')
ni_keys = [ni_key for ni_key in train_imgs]
train_data = np.array([train_imgs[ni_keys[i]] for i in range(len(ni_keys))])

train_labels = np.load('dist_train_lbls.npz')
nl_keys = [nl_key for nl_key in train_labels]
train_lbls = np.array([label_dict[str(train_labels[nl_keys[i]])] for i in range(len(nl_keys))])

test_imgs = np.load('dist_test_imgs.npz')
ti_keys = [ti_key for ti_key in test_imgs]
test_data = np.array([test_imgs[ti_keys[i]] for i in range(len(ti_keys))])

test_labels = np.load('dist_test_lbls.npz')
tl_keys = [tl_key for tl_key in test_labels]
test_lbls = np.array([label_dict[str(test_labels[tl_keys[i]])] for i in range(len(tl_keys))])

# reshaping data to have a color dimension of 1
train_data = np.reshape(train_data, (623, 110, 147, 1)).astype('float32') / 255
test_data = np.reshape(test_data, (77, 110, 147, 1)).astype('float32') / 255</code></pre>

                        Yeah, so I completely forgot the analysis won't work with actual categorical data, so the first thing I do here is create a quick dictionary mapping the
                        original labels I gave to the numbers 0-6. Now when I load in the labels, I convert the dictionary using the map and move on. Also notice I no longer need
                        any pickles, as I have split out the labels and the figures.
                    </p>
                </div>
            </section>

            <!-- Three -->
            <section id="TheModel">
                <div class="container">
                    <h3>The Model</h3>
                    <p>
                        OK, so there are a bunch of ways to go about doing this, but I am going to touch on two of them. The first is to try to use dense layers to connect
                        individual pixels to the final output. There are many issues with that, including major inefficiencies and the inability to capture larger features. I
                        went ahead and ran a dense layer for the fun of it, and the test results were 80% accurate.

<pre><code>model = keras.Sequential([
keras.layers.Flatten(),
keras.layers.Dense(512, activation = 'relu'),
keras.layers.Dense(256, activation = 'relu'),
keras.layers.Dense(7, activation = 'softmax')
])

model.compile(optimizer = 'adam',
loss = keras.losses.SparseCategoricalCrossentropy(),
metrics = ['accuracy'])

model.fit(train_data, train_lbls, epochs = 5, batch_size = 32)

model.evaluate(test_data, test_lbls)</code></pre>

                        <img src="images/Dense_Model.JPG" height="250" width="650" alt="Dense Model">
                        <br /><br />
                        So the main thing here is that the model reached 98% accuracy pretty rapidly for the training set, but it wasn't nearly as successful with the test data. For
                        these kinds of figures, as simple as they are, I would expect a better job overall. This is a common issue with dense layers in image classification, and so
                        most people use alternatives like the Convolutional Neural Networks.
                        <br /><br />

<pre><code>model = keras.Sequential([
keras.layers.Conv2D(64, 21, activation = 'relu', input_shape = (110, 147, 1)),
keras.layers.Conv2D(21, 7, activation = 'relu', input_shape = (110, 147, 1)),
keras.layers.Flatten(),
keras.layers.Dense(7, activation = 'softmax')
])

model.compile(optimizer = 'adam',
loss = keras.losses.SparseCategoricalCrossentropy(),
metrics = ['accuracy'])

model.fit(train_data, train_lbls, epochs = 5, batch_size = 32)

model.evaluate(test_data, test_lbls)</code></pre>

                        <img src="images/Convolution_Model.JPG" height="250" width="650" alt="CNN Model">
                        <br />
                        Now that's more what I'm talking about! At 95%, our model is doing about as well as most statistical tests, so I certainly can't complain.
                    </p>
                </div>
            </section>

            <!-- Four -->
            <section id="PostMortem">
                <div class="container">
                    <h3>Post Mortem</h3>
                    <p>
                        So there we have it, a quick little project to create an AI that can identify the level of skew in a distribution based on an image of its histogram.
                        Despite everything, I did learn a few good points here.
                        <br />
                        <br />
                        I think the biggest thing is honestly that I need to work on a more efficient setup for my work. Most of the work I've done in Python has been through
                        desktop IDEs like PyCharm. These are great for creating programs that sort image pixels in a spiraling pattern from the center, but they don't really work 
                        very well for machinge learning projects, especially when you have to rerun specific blocks of code over and over again. I really feel I should have used a 
                        Jupyter notebook for this or any other cell-based system.
                        <br />
                        <br />
                        I did still end up running into the pitfalls I feared, but they weren't too bad to get out of. The memory problem was the largest issue, and I do need to
                        find better routes for future, more complex projects. I could also consider using my better computer for this, but I would like to work on optimizations
                        first.
                        <br />
                        <br />
                        I will try to figure out some better techniques and apply them to the next project (maybe even a follow-up project?). Regardless, I feel like this was a
                        nice success, and I hope you enjoyed reading it.
                    </p>

                </div>
            </section>

        </div>

        <!-- Footer -->
        <section id="footer">
            <div class="container">
                <ul class="copyright">
                    <li>&copy; Untitled. All rights reserved.</li>
                    <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </section>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>
</html>